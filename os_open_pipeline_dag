'''
This is the script defining the dag OS_Open_Pipeline
'''

import os
from dotenv import load_dotenv
import requests
import sys
import logging
import pprint
import datetime as dt
import ast
import math
import urllib.parse
import zipfile
import shutil
import subprocess
# import sys

# airflow imports
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.python import ShortCircuitOperator
from airflow.models.baseoperator import chain
from airflow import models

# importing from master file
from os_open_pipeline import *

################################################################
# ShortCircuitOperator function

def skip_upload_check(ti=None):
    '''Checks the xcom produced by downloads to determine if any files should be uploaded to the shared drive'''
    # Get the XCom value
    xcom_value = ti.xcom_pull(key="download_count")

    # If the XCom value is equal to zero, skip all the next tasks
    if xcom_value == 0:
        return False

    # Otherwise, continue with the pipeline
    return True

################################################################
# Airflow variables

default_args = {
    'owner': 'airflow',
    'depends_on_past': True,
    'start_date': dt.datetime(2023,9,19,10,30,0),
    'email': ['user@domain.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0,
    'retry_delay': dt.timedelta(minutes=5)
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
    # 'dag': dag,
    # 'sla': timedelta(hours=2),
    # 'execution_timeout': timedelta(seconds=300),
    # 'on_failure_callback': some_function,
    # 'on_success_callback': some_other_function,
    # 'on_retry_callback': another_function,
    # 'sla_miss_callback': yet_another_function,
    # 'trigger_rule': 'all_success'
}

# creating the instance of the dag
dag = DAG(
    'OS_Open_Pipeline', 
    default_args=default_args,
    description='Connecting to OS Open API to check for updates, downloads them and uploads it to the shared drive',
    schedule=dt.timedelta(hours=4),
    catchup=False,
    max_active_runs=1,
    tags=['Ordnance Survey','Production','OS Open','Main Pipeline']
)

# Defining the task that downloads the update files
task1 = PythonOperator(
    task_id='download',
    python_callable=download_data,
    dag=dag,
    retries=3
)

# ShortCircuitOperator task that decides if the next task is run or not
skip_task_if_false = ShortCircuitOperator(
    task_id='skip_upload',
    dag=dag,
    python_callable=skip_upload_check,
    ignore_downstream_trigger_rules=True # ensures the subsequent tasks is skipped
)

# Defining the task that uploads the files to the shared drive
task2 = PythonOperator(
    task_id='upload_shareddrive',
    python_callable=upload,
    dag=dag,
    depends_on_past=True # ensures the data engineer checked that nothing is wrong with datasets before writing to datasets
)

# Dependancies
task1 >> skip_task_if_false >> task2
