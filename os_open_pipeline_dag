'''
This is the script defining the dag OS_Open_Pipeline, which is the full pipeline
It works by importing all the required modules from the os_open_functions file to create the dag
In order to look at or make edits to full pipeline, see os_open_functions
'''

import os
from dotenv import load_dotenv
import requests
import sys
import logging
import pprint
import datetime as dt
import ast
import math
import urllib.parse
import zipfile
import shutil
import subprocess
# import sys

# airflow imports
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.python import ShortCircuitOperator
from airflow.models.baseoperator import chain
from airflow import models

# importing from master file
from os_open_classes import *

# functions from global functions file
from global_functions import skip_upload_check

################################################################
# Airflow variables

default_args = {
    'owner': 'airflow',
    'depends_on_past': True,
    'start_date': dt.datetime(2023,9,19,10,30,0),
    'email': ['jan.salcedo@emapsite.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0,
    'retry_delay': dt.timedelta(minutes=5)
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
    # 'dag': dag,
    # 'sla': timedelta(hours=2),
    # 'execution_timeout': timedelta(seconds=300),
    # 'on_failure_callback': some_function,
    # 'on_success_callback': some_other_function,
    # 'on_retry_callback': another_function,
    # 'sla_miss_callback': yet_another_function,
    # 'trigger_rule': 'all_success'
}

# creating the instance of the dag
dag = DAG(
    'OS_Open_Pipeline', 
    default_args=default_args,
    description='Connecting to OS Open API to check for updates, downloads them and uploads it to the shared drive',
    schedule=dt.timedelta(hours=4),
    catchup=False,
    max_active_runs=1,
    tags=['Ordnance Survey','Production','OS Open','Main Pipeline']
)

# Defining the task that runs the version check
task1 = PythonOperator(
    task_id='version_check',
    python_callable=update_check,
    dag=dag,
    retries=3,
    depends_on_past=False
)

# Defining the task that downloads the update files
task2 = PythonOperator(
    task_id='download',
    python_callable=download_data,
    dag=dag,
    retries=3
)

# ShortCircuitOperator task that decides if the next task is run or not
skip_task_if_false = ShortCircuitOperator(
    task_id='skip_upload',
    dag=dag,
    python_callable=skip_upload_check,
    ignore_downstream_trigger_rules=True # ensures the subsequent tasks is skipped
)

# Defining the task that uploads the files to the shared drive
task3 = PythonOperator(
    task_id='upload_shareddrive',
    python_callable=upload,
    dag=dag,
    depends_on_past=True # ensures the data engineer checked that nothing is wrong with datasets before writing to datasets
)

# Dependancies
task1 >> task2 >> skip_task_if_false >> task3
